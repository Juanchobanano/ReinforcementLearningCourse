{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cart-Pole Game. Deep Q-Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Import libraries.\n",
    "import random\n",
    "from collections import deque\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow.compat.v1 as tf\n",
    "#tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the cart-pole environment.\n",
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\juanc\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3325, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-10-f6bf0016c982>\", line 4, in <module>\n",
      "    input_placeholder = tf.placeholder(\"float\", [None, input_size])\n",
      "AttributeError: module 'tensorflow' has no attribute 'placeholder'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\juanc\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2039, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'AttributeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\juanc\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
      "  File \"C:\\Users\\juanc\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n",
      "    _pywrap_tensorflow_internal = swig_import_helper()\n",
      "  File \"C:\\Users\\juanc\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n",
      "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n",
      "  File \"C:\\Users\\juanc\\Anaconda3\\lib\\imp.py\", line 242, in load_module\n",
      "    return load_dynamic(name, filename, file)\n",
      "  File \"C:\\Users\\juanc\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\n",
      "    return _load(spec)\n",
      "ImportError: DLL load failed: The specified module could not be found.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\juanc\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\juanc\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\juanc\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\juanc\\Anaconda3\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\juanc\\Anaconda3\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\juanc\\Anaconda3\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\juanc\\Anaconda3\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"C:\\Users\\juanc\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\Users\\juanc\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\Users\\juanc\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"C:\\Users\\juanc\\Anaconda3\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 42, in <module>\n",
      "    from . _api.v2 import audio\n",
      "  File \"C:\\Users\\juanc\\Anaconda3\\lib\\site-packages\\tensorflow_core\\_api\\v2\\audio\\__init__.py\", line 10, in <module>\n",
      "    from tensorflow.python.ops.gen_audio_ops import decode_wav\n",
      "  File \"C:\\Users\\juanc\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_audio_ops.py\", line 9, in <module>\n",
      "    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\n",
      "  File \"C:\\Users\\juanc\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\Users\\juanc\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\Users\\juanc\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"C:\\Users\\juanc\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\n",
      "    from tensorflow.python import pywrap_tensorflow\n",
      "  File \"C:\\Users\\juanc\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\n",
      "    raise ImportError(msg)\n",
      "ImportError: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\juanc\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3325, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-10-f6bf0016c982>\", line 4, in <module>\n",
      "    input_placeholder = tf.placeholder(\"float\", [None, input_size])\n",
      "AttributeError: module 'tensorflow' has no attribute 'placeholder'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\juanc\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2039, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'AttributeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\juanc\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
      "  File \"C:\\Users\\juanc\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n",
      "    _pywrap_tensorflow_internal = swig_import_helper()\n",
      "  File \"C:\\Users\\juanc\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n",
      "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n",
      "  File \"C:\\Users\\juanc\\Anaconda3\\lib\\imp.py\", line 242, in load_module\n",
      "    return load_dynamic(name, filename, file)\n",
      "  File \"C:\\Users\\juanc\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\n",
      "    return _load(spec)\n",
      "ImportError: DLL load failed: The specified module could not be found.\n",
      "\n",
      "\n",
      "Failed to load the native TensorFlow runtime.\n",
      "\n",
      "See https://www.tensorflow.org/install/errors\n",
      "\n",
      "for some common reasons and solutions.  Include the entire stack trace\n",
      "above this error message when asking for help.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'placeholder'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Build the network.\n",
    "input_size = env.observation_space.shape[0]\n",
    "\n",
    "input_placeholder = tf.placeholder(\"float\", [None, input_size])\n",
    "\n",
    "# Weights and bias of the hidden layer.\n",
    "weights_1 = tf.Variable(tf.truncated_normal([input_size, 20], stddev = 0.01))\n",
    "bias_1 = tf.Variable(tf.constant(0.0, shape = [20]))\n",
    "\n",
    "# Weights and bias of the output layer.\n",
    "weights_2 = tf.Variable(tf.truncated_normal([20, env.action_space.n], stddev = 0.01))\n",
    "bias_2 = tf.Variable(tf.constant(0.0, shape = [env.action_space.n]))\n",
    "\n",
    "hidden_layer = tf.nn.tanh(tf.matmul(input_placeholder, weights_1) + bias_1)\n",
    "output_layer = tf.matmul(hidden_layer, weights_2) + bias_2\n",
    "\n",
    "action_placeholder = tf.placeholder(\"float\", [None, 2])\n",
    "target_placeholder = tf.placeholder(\"float\", [None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network estimation.\n",
    "q_estimation = tf.reduce_sum(tf.multiply(output_layer, action_placeholder), reduction_indices = 1)\n",
    "\n",
    "# Loss function.\n",
    "loss = tf.reduce_mean(tf.square(target_placeholder - q_estimation))\n",
    "\n",
    "# Use Adam.\n",
    "train_operation = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "# Initialize TF variable.\n",
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_next_action(state, rand_action_prob):\n",
    "    \"\"\"\n",
    "    Simplified e-greedy policy.\n",
    "    :param state: current state\n",
    "    :param rand_action_prob: probability to select random action \n",
    "    \"\"\"\n",
    "    new_action = np.zeros([env.action_space.n])\n",
    "    if random.random() <= rand_action_prob: \n",
    "        # Choose an action randomly.\n",
    "        action_index = random.randrange(env.action_space.n)\n",
    "    else: \n",
    "        # Choose an action given our state.\n",
    "        action_values = session.run(output_layer, feed_dict = {input_placeholder: [state]})[0]\n",
    "\n",
    "        # We will take the highest value action.\n",
    "        action_index = np.argmax(action_values)\n",
    "        \n",
    "    new_action[action_index] = 1\n",
    "    return new_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(mini_batch):\n",
    "    \"\"\"\n",
    "    Train the network on a single minibatch\n",
    "    :param mini_batch the mini-batch\n",
    "    \"\"\"\n",
    "    last_state, last_action, reward, current_state, terminal = range(5)\n",
    "    \n",
    "    # Get the batch variables.\n",
    "    previous_states = [d[last_state] for d in mini_batch]\n",
    "    actions = [d[last_action] for d in mini_batch]\n",
    "    rewards = [d[reward] for d in mini_batch]\n",
    "    current_states = [d[current_state] for d in mini_batch]\n",
    "    agents_expected_reward = []\n",
    "    \n",
    "    # This gives us the agents expected reward for each action we might take.\n",
    "    agents_reward_per_action = session.run(output_layer, feed_dict = {input_placeholder: current_states})\n",
    "    for i in range(len(mini_batch)):\n",
    "        if mini_batch[i][terminal]:\n",
    "            # This was a terminal frame so there is no future reward...\n",
    "            agents_expected_reward.append(rewards[i])\n",
    "        else: \n",
    "            # Otherwise compute expected reward.\n",
    "            discount_factor = 0.9\n",
    "            agents_expected_reward.append(rewards[i] + discount_factor * np.max(agents_reward_per_action[i]))\n",
    "            \n",
    "    # Learn that these actions in these states lead to this reward.\n",
    "    session.run(train_operation, feed_dict = {\n",
    "        input_placeholder: previous_states, \n",
    "        action_placeholder: actions, \n",
    "        target_placeholder: agents_expected_reward\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(): \n",
    "    \"\"\"The Q-learning method\"\"\"\n",
    "    episode_lengths = list()\n",
    "    \n",
    "    # Experience replay buffer and definition.\n",
    "    observations = deque(maxlen = 200000)\n",
    "    \n",
    "    # Set the first action to nothing.\n",
    "    last_action = np.zeros(env.action_space.n)\n",
    "    last_action[1] = 1\n",
    "    last_state = env.reset()\n",
    "    \n",
    "    total_reward = 0\n",
    "    episode = 1\n",
    "    \n",
    "    time_step = 0\n",
    "    \n",
    "    # Initial chance to select random action.\n",
    "    rand_action_prob = 1.0\n",
    "    \n",
    "    while episode <= 500: \n",
    "        # Render the cart pole on the screen.\n",
    "        \n",
    "        if episode >= 300:\n",
    "            env.render()\n",
    "        \n",
    "        # Select action following the policy.\n",
    "        last_action = choose_next_action(last_state, rand_action_prob)\n",
    "        \n",
    "        # Take action and receive new state and reward.\n",
    "        current_state, reward, terminal, info = env.step(np.argmax(last_action))\n",
    "        total_reward += reward\n",
    "        \n",
    "        if terminal: \n",
    "            reward = -1\n",
    "            episode_lengths.append(time_step)\n",
    "            \n",
    "            #print(\"Episode: %s; Steps before fail: %s; Epsilon: %.2f reward %s\" % (episode, time_step, rand_action_prob, total_reward))\n",
    "            total_reward = 0\n",
    "            \n",
    "        # Store the transition in previous observations.\n",
    "        observations.append((last_state, last_action, reward, current_state, terminal))\n",
    "        \n",
    "        # Only train if done observing.\n",
    "        min_experience_replay_size = 5000\n",
    "        if len(observations) > min_experience_replay_size: \n",
    "            # Mini batch of 128 from the experience replay observations.\n",
    "            mini_batch = random.sample(observations, 128)\n",
    "            \n",
    "            # Train the network.\n",
    "            train(mini_batch)\n",
    "            \n",
    "            time_step += 1\n",
    "        \n",
    "        # Reset the environment.\n",
    "        if terminal: \n",
    "            last_state = env.reset()\n",
    "            time_step = 0\n",
    "            episode += 1\n",
    "        else: \n",
    "            last_state = current_state\n",
    "            \n",
    "        # Gradually reduce the probability of a random action\n",
    "        # Starting from 1 and going to 0\n",
    "        if rand_action_prob > 0  and len(observations) > min_experience_replay_size: \n",
    "            rand_action_prob -= 1.0 / 15000\n",
    "            \n",
    "    # Display episodes length.\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Length (steps)\")\n",
    "    plt.plot(episode_lengths, label = \"Episode length\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_learning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
